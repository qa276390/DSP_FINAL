{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.4\n",
      "1.0.1.post2\n",
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "# import some libraries you maybe use\n",
    "import torchvision # an useful library to help I/O (highly recommend). To install this, just do \"pip install torchvision\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "import os\n",
    "import visdom\n",
    "import scipy\n",
    "from utils.transform import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './results'\n",
    "TRAINING_NAME = 'resnet50_melpaddata'\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, TRAINING_NAME)\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, 'weight.pth')\n",
    "ACC_MODEL_PATH = os.path.join(OUTPUT_DIR, 'weight_acc.pth')\n",
    "RESULT_PATH = os.path.join(OUTPUT_DIR, 'result.csv')\n",
    "NP_PATH =  os.path.join(OUTPUT_DIR, 'raw_result.npy')\n",
    "LOG_PATH = os.path.join(OUTPUT_DIR, 'log')\n",
    "isPAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "log = open(LOG_PATH, \"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "In order to train the model with training data, the first step is to read the data from your folder, database, etc. The below is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Resize, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# Define path to your dataset\n",
    "dataset = \"./melpaddata\" # the root folder\n",
    "trainpath = os.path.join(dataset,\"train\") # train set\n",
    "valpath = os.path.join(dataset,\"val\") # validation set\n",
    "\"\"\"\n",
    "def pad(raw):\n",
    "    ex = np.zeros((22050,))\n",
    "    ex[:len(raw)] = raw\n",
    "    return ex\n",
    "\n",
    "cut = lambda x: x[-11025:]\n",
    "#cut_front = lambda x: x[:11025]\n",
    "norm =  lambda x: (x.astype(np.float32) / (np.max(x)+1e-6))*0.5\n",
    "spct = lambda x: scipy.signal.spectrogram(x ,fs= 10e3,mode='phase')[2] #overlap\n",
    "tri = lambda x: [x, x, x]\n",
    "totensor = lambda x: torch.Tensor(x)\n",
    "\n",
    "tsfm = Compose([\n",
    "        pad, # rescale to -1 to 1\n",
    "        norm, # rescale to -1 to 1\n",
    "        spct, # MFCC \n",
    "        tri,\n",
    "        totensor\n",
    "        ])\n",
    "\n",
    "nploader = np.load\n",
    "\"\"\"\n",
    "torchloader = torch.load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DatasetFolder(root=trainpath, loader=torchloader, extensions=['pt'])\n",
    "valdata = DatasetFolder(root=valpath, loader=torchloader, extensions=['pt'])\n",
    "\n",
    "# Create a loader\n",
    "trainloader = DataLoader(traindata,batch_size=batch_size,shuffle=True, pin_memory=True, num_workers=6)\n",
    "valloader = DataLoader(valdata,batch_size=batch_size,shuffle=True,  pin_memory=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frog1', 'Frog2', 'Frog3', 'Grylloidea1', 'Grylloidea2', 'Tettigonioidea1', 'Tettigonioidea2', 'drums_FloorTom', 'drums_HiHat', 'drums_Kick', 'drums_MidTom', 'drums_Ride', 'drums_Rim', 'drums_SmallTom', 'drums_Snare', 'guitar_3rd_fret', 'guitar_7th_fret', 'guitar_9th_fret', 'guitar_chord1', 'guitar_chord2']\n",
      "{'Frog1': 0, 'Frog2': 1, 'Frog3': 2, 'Grylloidea1': 3, 'Grylloidea2': 4, 'Tettigonioidea1': 5, 'Tettigonioidea2': 6, 'drums_FloorTom': 7, 'drums_HiHat': 8, 'drums_Kick': 9, 'drums_MidTom': 10, 'drums_Ride': 11, 'drums_Rim': 12, 'drums_SmallTom': 13, 'drums_Snare': 14, 'guitar_3rd_fret': 15, 'guitar_7th_fret': 16, 'guitar_9th_fret': 17, 'guitar_chord1': 18, 'guitar_chord2': 19}\n"
     ]
    }
   ],
   "source": [
    "print(traindata.classes) # show all classes\n",
    "print(traindata.class_to_idx) # show the mapping from class to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Frog1', 1: 'Frog2', 2: 'Frog3', 3: 'Grylloidea1', 4: 'Grylloidea2', 5: 'Tettigonioidea1', 6: 'Tettigonioidea2', 7: 'drums_FloorTom', 8: 'drums_HiHat', 9: 'drums_Kick', 10: 'drums_MidTom', 11: 'drums_Ride', 12: 'drums_Rim', 13: 'drums_SmallTom', 14: 'drums_Snare', 15: 'guitar_3rd_fret', 16: 'guitar_7th_fret', 17: 'guitar_9th_fret', 18: 'guitar_chord1', 19: 'guitar_chord2'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_class = {val: key for key, val in traindata.class_to_idx.items()} # build an inverse mapping for later use\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx2class = {9: 'Frog1', 10: 'Frog2', 19: 'Frog3', 3: 'Grylloidea1', 14: 'Grylloidea2', 0: 'Tettigonioidea1', 1: 'Tettigonioidea2', 11: 'drums_FloorTom', 5: 'drums_HiHat', 6: 'drums_Kick', 4: 'drums_MidTom', 16: 'drums_Ride', 13: 'drums_Rim', 7: 'drums_SmallTom', 2: 'drums_Snare', 15: 'guitar_3rd_fret', 12: 'guitar_7th_fret', 18: 'guitar_9th_fret', 17: 'guitar_chord1', 8: 'guitar_chord2'}\n",
    "#print(correct_idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class2idx = {val: key for key, val in correct_idx2class.items()}\n",
    "#print(correct_class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 10, 2: 19, 3: 3, 4: 14, 5: 0, 6: 1, 7: 11, 8: 5, 9: 6, 10: 4, 11: 16, 12: 13, 13: 7, 14: 2, 15: 15, 16: 12, 17: 18, 18: 17, 19: 8}\n"
     ]
    }
   ],
   "source": [
    "corrected_idx2idx = {val: correct_class2idx[key] for key, val in traindata.class_to_idx.items()}\n",
    "print(corrected_idx2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an example network\n",
    "If you're unfamiliar with this part, please see the HW1 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.resnet as resnet\n",
    "model =resnet.resnet50(num_classes= len(traindata.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda now!\n"
     ]
    }
   ],
   "source": [
    "#net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.01) # setup your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss() # setup your criterion\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8) #0.1->0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "    for key in state: state[key] = state[key].clone().cpu()\n",
    "    torch.save(state, filename)\n",
    "#save_model(net,\"weight.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import utils.visdoms as visdoms\n",
    "plotter = visdoms.VisdomLinePlotter(env_name=TRAINING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................Val Acc: 0.559380\n",
      "epoch 1, lr 0.010000, loss: 1.6108, val_loss: 1.9892\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "num_epoch = 150\n",
    "best_loss = 1e8\n",
    "best_acc = 0\n",
    "train_losses = visdoms.AverageMeter()\n",
    "val_losses = visdoms.AverageMeter()\n",
    "for epoch in range(num_epoch):\n",
    "    #net.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        #print(data.shape,target)\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_losses.update(loss.data.cpu().numpy(), target.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('.',  end='')\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    plotter.plot('loss', 'train', 'Class Loss', epoch, train_losses.avg)\n",
    "    \n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(valloader):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            output = net(data)\n",
    "            val_loss = criterion(output, target)\n",
    "            val_losses.update(val_loss.data.cpu().numpy(), target.size(0))\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            print('.',  end='')\n",
    "        acc = correct.item() / len(valloader.dataset)\n",
    "        print(\"Val Acc: %f\"%(acc))\n",
    "\n",
    "        plotter.plot('loss', 'val', 'Class Loss', epoch, val_losses.avg)\n",
    "        plotter.plot('acc', 'val', 'Class Accuracy', epoch, acc)\n",
    "            \n",
    "        print('epoch %d, lr %.6f, loss: %.4f, val_loss: %.4f' %(epoch+1, optimizer.param_groups[0]['lr'], loss.item(), val_loss.item()))\n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            save_model(net, MODEL_PATH)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            save_model(net, ACC_MODEL_PATH)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses.arr)\n",
    "plt.plot(val_losses.arr)\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model\n",
    "#net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "net = load_model(net, MODEL_PATH)\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(valloader):\n",
    "        #print(type(data))\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    acc = correct.item() / len(valloader.dataset)\n",
    "print(\"Validation Classification Accuracy: %f\"%(acc))\n",
    "log.write('Validation Classification Accuracy: %f\"%(acc)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('./data/test.npy', allow_pickle=True)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_spec = []\n",
    "for t in test_data:\n",
    "    \"\"\"\n",
    "    test = cut(t)\n",
    "    test = norm(test)\n",
    "    test = spct(test) \n",
    "    test = tri(test)\n",
    "    test = totensor(test)\n",
    "    \"\"\"\n",
    "    test = transform(t, padding=isPAD)\n",
    "    t_spec.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test))\n",
    "print(type(test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.stack(t_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(tensor_x) # create your datset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "result = []\n",
    "raw_result = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, ) in enumerate(test_dataloader):  \n",
    "        data = data.to(device)\n",
    "        #target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        result = result + list(pred.cpu().numpy().ravel())\n",
    "        raw_result.append(list(output.cpu().numpy().ravel()))\n",
    "    #acc = correct.item() / len(valloader.dataset)\n",
    "#print(\"Validation Classification Accuracy: %f\"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(NP_PATH, np.asarray(raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [corrected_idx2idx[idx] for idx in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'id': list(range(0 ,len(test_data))), 'category':result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(RESULT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
