{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1,2,3 Classification\n",
    "In this tutorial, I'll go throgh some implementations details you maybe use in your final project:\n",
    "1. how to load the dataset\n",
    "2. how to use pre-processing\n",
    "3. how to train the model\n",
    "4. how to save and load your well-trained model\n",
    "5. how to test your performance\n",
    "6. how to obtain predictions from a few images\n",
    "7. how to visual your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n",
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "# import some libraries you maybe use\n",
    "import torchvision # an useful library to help I/O (highly recommend). To install this, just do \"pip install torchvision\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "In order to train the model with training data, the first step is to read the data from your folder, database, etc. The below is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Resize, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import librosa\n",
    "print('load done')\n",
    "# Define path to your dataset\n",
    "dataset = \"./data\" # the root folder\n",
    "trainpath = os.path.join(dataset,\"train\") # train set\n",
    "valpath = os.path.join(dataset,\"val\") # validation set\n",
    "\n",
    "cut = lambda x: x[:11025]\n",
    "norm =  lambda x: x.astype(np.float32) / np.max(x)\n",
    "spct = lambda x: [librosa.feature.melspectrogram(x, sr=44100),librosa.feature.melspectrogram(x, sr=44100),librosa.feature.melspectrogram(x, sr=44100)]\n",
    "totensor = lambda x: torch.Tensor(x)\n",
    "\n",
    "tsfm = Compose([\n",
    "        cut, # rescale to -1 to 1\n",
    "        norm, # rescale to -1 to 1\n",
    "        spct, # MFCC \n",
    "        totensor\n",
    "        ])\n",
    "\n",
    "# Define some operations to preprocess your inputs.\n",
    "#mytransforms = Compose([Grayscale(num_output_channels=1),Resize((32,32)),ToTensor()])\n",
    "nploader = np.load\n",
    "# The above line will work in this flow:\n",
    "# (PIL_RGB_INPUT) => (PIL_GrayScale_INPUT) => (32x32_PIL_GrayScale_INPUT) => (32x32_Tensor_GrayScale_INPUT)\n",
    "\n",
    "# Create imagefolder object.\n",
    "# The ImageFolder(...) is a powerful class to load the data from the folders.\n",
    "# The data should be arranged in this manner:\n",
    "# root/dog/xxx.png\n",
    "# root/dog/xxy.png\n",
    "# root/dog/xxz.png\n",
    "# ...\n",
    "# root/cat/123.png\n",
    "# root/cat/nsdf3.png\n",
    "# root/cat/asd932_.png\n",
    "# =============================================\n",
    "# To learn more, please visit the website:\n",
    "# 1. Official API LINK: https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n",
    "# 2. Good Explaination LINK: https://discuss.pytorch.org/t/questions-about-imagefolder/774/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DatasetFolder(root=trainpath, loader=nploader, transform=tsfm, extensions=['npy'])\n",
    "valdata = DatasetFolder(root=valpath, loader=nploader, transform=tsfm, extensions=['npy'])\n",
    "\n",
    "# Create a loader\n",
    "trainloader = DataLoader(traindata,batch_size=batch_size,shuffle=True)\n",
    "valloader = DataLoader(valdata,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frog1', 'Frog2', 'Frog3', 'Grylloidea1', 'Grylloidea2', 'Tettigonioidea1', 'Tettigonioidea2', 'drums_FloorTom', 'drums_HiHat', 'drums_Kick', 'drums_MidTom', 'drums_Ride', 'drums_Rim', 'drums_SmallTom', 'drums_Snare', 'guitar_3rd_fret', 'guitar_7th_fret', 'guitar_9th_fret', 'guitar_chord1', 'guitar_chord2']\n",
      "{'Frog1': 0, 'Frog2': 1, 'Frog3': 2, 'Grylloidea1': 3, 'Grylloidea2': 4, 'Tettigonioidea1': 5, 'Tettigonioidea2': 6, 'drums_FloorTom': 7, 'drums_HiHat': 8, 'drums_Kick': 9, 'drums_MidTom': 10, 'drums_Ride': 11, 'drums_Rim': 12, 'drums_SmallTom': 13, 'drums_Snare': 14, 'guitar_3rd_fret': 15, 'guitar_7th_fret': 16, 'guitar_9th_fret': 17, 'guitar_chord1': 18, 'guitar_chord2': 19}\n"
     ]
    }
   ],
   "source": [
    "print(traindata.classes) # show all classes\n",
    "print(traindata.class_to_idx) # show the mapping from class to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Frog1', 1: 'Frog2', 2: 'Frog3', 3: 'Grylloidea1', 4: 'Grylloidea2', 5: 'Tettigonioidea1', 6: 'Tettigonioidea2', 7: 'drums_FloorTom', 8: 'drums_HiHat', 9: 'drums_Kick', 10: 'drums_MidTom', 11: 'drums_Ride', 12: 'drums_Rim', 13: 'drums_SmallTom', 14: 'drums_Snare', 15: 'guitar_3rd_fret', 16: 'guitar_7th_fret', 17: 'guitar_9th_fret', 18: 'guitar_chord1', 19: 'guitar_chord2'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_class = {val: key for key, val in traindata.class_to_idx.items()} # build an inverse mapping for later use\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: 'Frog1', 10: 'Frog2', 19: 'Frog3', 3: 'Grylloidea1', 14: 'Grylloidea2', 0: 'Tettigonioidea1', 1: 'Tettigonioidea2', 11: 'drums_FloorTom', 5: 'drums_HiHat', 6: 'drums_Kick', 4: 'drums_MidTom', 16: 'drums_Ride', 13: 'drums_Rim', 7: 'drums_SmallTom', 2: 'drums_Snare', 15: 'guitar_3rd_fret', 12: 'guitar_7th_fret', 18: 'guitar_9th_fret', 17: 'guitar_chord1', 8: 'guitar_chord2'}\n"
     ]
    }
   ],
   "source": [
    "correct_idx2class = {9: 'Frog1', 10: 'Frog2', 19: 'Frog3', 3: 'Grylloidea1', 14: 'Grylloidea2', 0: 'Tettigonioidea1', 1: 'Tettigonioidea2', 11: 'drums_FloorTom', 5: 'drums_HiHat', 6: 'drums_Kick', 4: 'drums_MidTom', 16: 'drums_Ride', 13: 'drums_Rim', 7: 'drums_SmallTom', 2: 'drums_Snare', 15: 'guitar_3rd_fret', 12: 'guitar_7th_fret', 18: 'guitar_9th_fret', 17: 'guitar_chord1', 8: 'guitar_chord2'}\n",
    "print(correct_idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Frog1': 9, 'Frog2': 10, 'Frog3': 19, 'Grylloidea1': 3, 'Grylloidea2': 14, 'Tettigonioidea1': 0, 'Tettigonioidea2': 1, 'drums_FloorTom': 11, 'drums_HiHat': 5, 'drums_Kick': 6, 'drums_MidTom': 4, 'drums_Ride': 16, 'drums_Rim': 13, 'drums_SmallTom': 7, 'drums_Snare': 2, 'guitar_3rd_fret': 15, 'guitar_7th_fret': 12, 'guitar_9th_fret': 18, 'guitar_chord1': 17, 'guitar_chord2': 8}\n"
     ]
    }
   ],
   "source": [
    "correct_class2idx = {val: key for key, val in correct_idx2class.items()}\n",
    "print(correct_class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 10, 2: 19, 3: 3, 4: 14, 5: 0, 6: 1, 7: 11, 8: 5, 9: 6, 10: 4, 11: 16, 12: 13, 13: 7, 14: 2, 15: 15, 16: 12, 17: 18, 18: 17, 19: 8}\n"
     ]
    }
   ],
   "source": [
    "corrected_idx2idx = {val: correct_class2idx[key] for key, val in traindata.class_to_idx.items()}\n",
    "print(corrected_idx2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an example network\n",
    "If you're unfamiliar with this part, please see the HW1 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "model =resnet.resnet18(num_classes= len(traindata.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out  = F.interpolate(out, size=(5, 5), mode='bilinear')  # resize to the size expected by the linear unit\n",
    "        out = out.view(out.size(0), 5 * 5 * 16)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda now!\n"
     ]
    }
   ],
   "source": [
    "net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05) # setup your optimizer\n",
    "criterion = nn.CrossEntropyLoss() # setup your criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "    for key in state: state[key] = state[key].clone().cpu()\n",
    "    torch.save(state, filename)\n",
    "#save_model(net,\"weight.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, iter 1 loss: 3.284\n",
      "epoch 1, iter 6 loss: 1.842\n",
      "epoch 1, iter 11 loss: 1.351\n",
      "epoch 1, iter 16 loss: 0.915\n",
      "epoch 1, iter 21 loss: 0.709\n",
      "epoch 1, iter 26 loss: 0.674\n",
      "epoch 1, iter 31 loss: 0.518\n",
      "epoch 1, iter 36 loss: 0.436\n",
      "epoch 1, iter 41 loss: 0.341\n",
      "epoch 2, iter 1 loss: 0.370\n",
      "epoch 2, iter 6 loss: 0.315\n",
      "epoch 2, iter 11 loss: 0.240\n",
      "epoch 2, iter 16 loss: 0.180\n",
      "epoch 2, iter 21 loss: 0.141\n",
      "epoch 2, iter 26 loss: 0.157\n",
      "epoch 2, iter 31 loss: 0.177\n",
      "epoch 2, iter 36 loss: 0.109\n",
      "epoch 2, iter 41 loss: 0.128\n",
      "epoch 3, iter 1 loss: 0.291\n",
      "epoch 3, iter 6 loss: 0.116\n",
      "epoch 3, iter 11 loss: 0.091\n",
      "epoch 3, iter 16 loss: 0.055\n",
      "epoch 3, iter 21 loss: 0.061\n",
      "epoch 3, iter 26 loss: 0.046\n",
      "epoch 3, iter 31 loss: 0.090\n",
      "epoch 3, iter 36 loss: 0.077\n",
      "epoch 3, iter 41 loss: 0.072\n",
      "epoch 4, iter 1 loss: 0.046\n",
      "epoch 4, iter 6 loss: 0.058\n",
      "epoch 4, iter 11 loss: 0.044\n",
      "epoch 4, iter 16 loss: 0.050\n",
      "epoch 4, iter 21 loss: 0.051\n",
      "epoch 4, iter 26 loss: 0.038\n",
      "epoch 4, iter 31 loss: 0.029\n",
      "epoch 4, iter 36 loss: 0.051\n",
      "epoch 4, iter 41 loss: 0.036\n",
      "epoch 5, iter 1 loss: 0.065\n",
      "epoch 5, iter 6 loss: 0.020\n",
      "epoch 5, iter 11 loss: 0.050\n",
      "epoch 5, iter 16 loss: 0.028\n",
      "epoch 5, iter 21 loss: 0.042\n",
      "epoch 5, iter 26 loss: 0.040\n",
      "epoch 5, iter 31 loss: 0.039\n",
      "epoch 5, iter 36 loss: 0.037\n",
      "epoch 5, iter 41 loss: 0.022\n",
      "epoch 6, iter 1 loss: 0.054\n",
      "epoch 6, iter 6 loss: 0.054\n",
      "epoch 6, iter 11 loss: 0.019\n",
      "epoch 6, iter 16 loss: 0.024\n",
      "epoch 6, iter 21 loss: 0.028\n",
      "epoch 6, iter 26 loss: 0.036\n",
      "epoch 6, iter 31 loss: 0.024\n",
      "epoch 6, iter 36 loss: 0.010\n",
      "epoch 6, iter 41 loss: 0.012\n",
      "epoch 7, iter 1 loss: 0.013\n",
      "epoch 7, iter 6 loss: 0.012\n",
      "epoch 7, iter 11 loss: 0.015\n",
      "epoch 7, iter 16 loss: 0.013\n",
      "epoch 7, iter 21 loss: 0.010\n",
      "epoch 7, iter 26 loss: 0.021\n",
      "epoch 7, iter 31 loss: 0.017\n",
      "epoch 7, iter 36 loss: 0.018\n",
      "epoch 7, iter 41 loss: 0.020\n",
      "epoch 8, iter 1 loss: 0.033\n",
      "epoch 8, iter 6 loss: 0.033\n",
      "epoch 8, iter 11 loss: 0.015\n",
      "epoch 8, iter 16 loss: 0.018\n",
      "epoch 8, iter 21 loss: 0.013\n",
      "epoch 8, iter 26 loss: 0.020\n",
      "epoch 8, iter 31 loss: 0.009\n",
      "epoch 8, iter 36 loss: 0.008\n",
      "epoch 8, iter 41 loss: 0.024\n",
      "epoch 9, iter 1 loss: 0.017\n",
      "epoch 9, iter 6 loss: 0.007\n",
      "epoch 9, iter 11 loss: 0.013\n",
      "epoch 9, iter 16 loss: 0.004\n",
      "epoch 9, iter 21 loss: 0.010\n",
      "epoch 9, iter 26 loss: 0.013\n",
      "epoch 9, iter 31 loss: 0.010\n",
      "epoch 9, iter 36 loss: 0.008\n",
      "epoch 9, iter 41 loss: 0.006\n",
      "epoch 10, iter 1 loss: 0.005\n",
      "epoch 10, iter 6 loss: 0.007\n",
      "epoch 10, iter 11 loss: 0.008\n",
      "epoch 10, iter 16 loss: 0.013\n",
      "epoch 10, iter 21 loss: 0.004\n",
      "epoch 10, iter 26 loss: 0.007\n",
      "epoch 10, iter 31 loss: 0.015\n",
      "epoch 10, iter 36 loss: 0.004\n",
      "epoch 10, iter 41 loss: 0.014\n",
      "epoch 11, iter 1 loss: 0.027\n",
      "epoch 11, iter 6 loss: 0.007\n",
      "epoch 11, iter 11 loss: 0.007\n",
      "epoch 11, iter 16 loss: 0.009\n",
      "epoch 11, iter 21 loss: 0.003\n",
      "epoch 11, iter 26 loss: 0.009\n",
      "epoch 11, iter 31 loss: 0.005\n",
      "epoch 11, iter 36 loss: 0.007\n",
      "epoch 11, iter 41 loss: 0.008\n",
      "epoch 12, iter 1 loss: 0.007\n",
      "epoch 12, iter 6 loss: 0.011\n",
      "epoch 12, iter 11 loss: 0.004\n",
      "epoch 12, iter 16 loss: 0.004\n",
      "epoch 12, iter 21 loss: 0.004\n",
      "epoch 12, iter 26 loss: 0.003\n",
      "epoch 12, iter 31 loss: 0.004\n",
      "epoch 12, iter 36 loss: 0.003\n",
      "epoch 12, iter 41 loss: 0.003\n",
      "epoch 13, iter 1 loss: 0.005\n",
      "epoch 13, iter 6 loss: 0.003\n",
      "epoch 13, iter 11 loss: 0.003\n",
      "epoch 13, iter 16 loss: 0.004\n",
      "epoch 13, iter 21 loss: 0.003\n",
      "epoch 13, iter 26 loss: 0.004\n",
      "epoch 13, iter 31 loss: 0.003\n",
      "epoch 13, iter 36 loss: 0.002\n",
      "epoch 13, iter 41 loss: 0.004\n",
      "epoch 14, iter 1 loss: 0.002\n",
      "epoch 14, iter 6 loss: 0.004\n",
      "epoch 14, iter 11 loss: 0.003\n",
      "epoch 14, iter 16 loss: 0.005\n",
      "epoch 14, iter 21 loss: 0.002\n",
      "epoch 14, iter 26 loss: 0.003\n",
      "epoch 14, iter 31 loss: 0.006\n",
      "epoch 14, iter 36 loss: 0.003\n",
      "epoch 14, iter 41 loss: 0.002\n",
      "epoch 15, iter 1 loss: 0.005\n",
      "epoch 15, iter 6 loss: 0.007\n",
      "epoch 15, iter 11 loss: 0.003\n",
      "epoch 15, iter 16 loss: 0.003\n",
      "epoch 15, iter 21 loss: 0.002\n",
      "epoch 15, iter 26 loss: 0.002\n",
      "epoch 15, iter 31 loss: 0.002\n",
      "epoch 15, iter 36 loss: 0.002\n",
      "epoch 15, iter 41 loss: 0.003\n",
      "epoch 16, iter 1 loss: 0.002\n",
      "epoch 16, iter 6 loss: 0.002\n",
      "epoch 16, iter 11 loss: 0.003\n",
      "epoch 16, iter 16 loss: 0.004\n",
      "epoch 16, iter 21 loss: 0.003\n",
      "epoch 16, iter 26 loss: 0.005\n",
      "epoch 16, iter 31 loss: 0.003\n",
      "epoch 16, iter 36 loss: 0.006\n",
      "epoch 16, iter 41 loss: 0.003\n",
      "epoch 17, iter 1 loss: 0.002\n",
      "epoch 17, iter 6 loss: 0.002\n",
      "epoch 17, iter 11 loss: 0.002\n",
      "epoch 17, iter 16 loss: 0.002\n",
      "epoch 17, iter 21 loss: 0.006\n",
      "epoch 17, iter 26 loss: 0.002\n",
      "epoch 17, iter 31 loss: 0.002\n",
      "epoch 17, iter 36 loss: 0.001\n",
      "epoch 17, iter 41 loss: 0.002\n",
      "epoch 18, iter 1 loss: 0.002\n",
      "epoch 18, iter 6 loss: 0.002\n",
      "epoch 18, iter 11 loss: 0.002\n",
      "epoch 18, iter 16 loss: 0.003\n",
      "epoch 18, iter 21 loss: 0.008\n",
      "epoch 18, iter 26 loss: 0.002\n",
      "epoch 18, iter 31 loss: 0.003\n",
      "epoch 18, iter 36 loss: 0.007\n",
      "epoch 18, iter 41 loss: 0.003\n",
      "epoch 19, iter 1 loss: 0.002\n",
      "epoch 19, iter 6 loss: 0.004\n",
      "epoch 19, iter 11 loss: 0.003\n",
      "epoch 19, iter 16 loss: 0.002\n",
      "epoch 19, iter 21 loss: 0.001\n",
      "epoch 19, iter 26 loss: 0.002\n",
      "epoch 19, iter 31 loss: 0.002\n",
      "epoch 19, iter 36 loss: 0.003\n",
      "epoch 19, iter 41 loss: 0.003\n",
      "epoch 20, iter 1 loss: 0.002\n",
      "epoch 20, iter 6 loss: 0.001\n",
      "epoch 20, iter 11 loss: 0.002\n",
      "epoch 20, iter 16 loss: 0.002\n",
      "epoch 20, iter 21 loss: 0.002\n",
      "epoch 20, iter 26 loss: 0.002\n",
      "epoch 20, iter 31 loss: 0.002\n",
      "epoch 20, iter 36 loss: 0.003\n",
      "epoch 20, iter 41 loss: 0.001\n",
      "epoch 21, iter 1 loss: 0.001\n",
      "epoch 21, iter 6 loss: 0.001\n",
      "epoch 21, iter 11 loss: 0.001\n",
      "epoch 21, iter 16 loss: 0.001\n",
      "epoch 21, iter 21 loss: 0.002\n",
      "epoch 21, iter 26 loss: 0.001\n",
      "epoch 21, iter 31 loss: 0.002\n",
      "epoch 21, iter 36 loss: 0.001\n",
      "epoch 21, iter 41 loss: 0.001\n",
      "epoch 22, iter 1 loss: 0.028\n",
      "epoch 22, iter 6 loss: 0.002\n",
      "epoch 22, iter 11 loss: 0.002\n",
      "epoch 22, iter 16 loss: 0.002\n",
      "epoch 22, iter 21 loss: 0.001\n",
      "epoch 22, iter 26 loss: 0.001\n",
      "epoch 22, iter 31 loss: 0.002\n",
      "epoch 22, iter 36 loss: 0.002\n",
      "epoch 22, iter 41 loss: 0.002\n",
      "epoch 23, iter 1 loss: 0.001\n",
      "epoch 23, iter 6 loss: 0.002\n",
      "epoch 23, iter 11 loss: 0.001\n",
      "epoch 23, iter 16 loss: 0.001\n",
      "epoch 23, iter 21 loss: 0.002\n",
      "epoch 23, iter 26 loss: 0.001\n",
      "epoch 23, iter 31 loss: 0.001\n",
      "epoch 23, iter 36 loss: 0.001\n",
      "epoch 23, iter 41 loss: 0.002\n",
      "epoch 24, iter 1 loss: 0.002\n",
      "epoch 24, iter 6 loss: 0.002\n",
      "epoch 24, iter 11 loss: 0.003\n",
      "epoch 24, iter 16 loss: 0.001\n",
      "epoch 24, iter 21 loss: 0.001\n",
      "epoch 24, iter 26 loss: 0.001\n",
      "epoch 24, iter 31 loss: 0.001\n",
      "epoch 24, iter 36 loss: 0.002\n",
      "epoch 24, iter 41 loss: 0.004\n",
      "epoch 25, iter 1 loss: 0.001\n",
      "epoch 25, iter 6 loss: 0.001\n",
      "epoch 25, iter 11 loss: 0.001\n",
      "epoch 25, iter 16 loss: 0.001\n",
      "epoch 25, iter 21 loss: 0.001\n",
      "epoch 25, iter 26 loss: 0.001\n",
      "epoch 25, iter 31 loss: 0.002\n",
      "epoch 25, iter 36 loss: 0.001\n",
      "epoch 25, iter 41 loss: 0.001\n",
      "epoch 26, iter 1 loss: 0.001\n",
      "epoch 26, iter 6 loss: 0.001\n",
      "epoch 26, iter 11 loss: 0.001\n",
      "epoch 26, iter 16 loss: 0.001\n",
      "epoch 26, iter 21 loss: 0.001\n",
      "epoch 26, iter 26 loss: 0.001\n",
      "epoch 26, iter 31 loss: 0.001\n",
      "epoch 26, iter 36 loss: 0.001\n",
      "epoch 26, iter 41 loss: 0.001\n",
      "epoch 27, iter 1 loss: 0.001\n",
      "epoch 27, iter 6 loss: 0.001\n",
      "epoch 27, iter 11 loss: 0.001\n",
      "epoch 27, iter 16 loss: 0.002\n",
      "epoch 27, iter 21 loss: 0.001\n",
      "epoch 27, iter 26 loss: 0.001\n",
      "epoch 27, iter 31 loss: 0.001\n",
      "epoch 27, iter 36 loss: 0.001\n",
      "epoch 27, iter 41 loss: 0.001\n",
      "epoch 28, iter 1 loss: 0.001\n",
      "epoch 28, iter 6 loss: 0.001\n",
      "epoch 28, iter 11 loss: 0.001\n",
      "epoch 28, iter 16 loss: 0.001\n",
      "epoch 28, iter 21 loss: 0.001\n",
      "epoch 28, iter 26 loss: 0.001\n",
      "epoch 28, iter 31 loss: 0.003\n",
      "epoch 28, iter 36 loss: 0.001\n",
      "epoch 28, iter 41 loss: 0.000\n",
      "epoch 29, iter 1 loss: 0.003\n",
      "epoch 29, iter 6 loss: 0.001\n",
      "epoch 29, iter 11 loss: 0.004\n",
      "epoch 29, iter 16 loss: 0.001\n",
      "epoch 29, iter 21 loss: 0.001\n",
      "epoch 29, iter 26 loss: 0.001\n",
      "epoch 29, iter 31 loss: 0.002\n",
      "epoch 29, iter 36 loss: 0.001\n",
      "epoch 29, iter 41 loss: 0.001\n",
      "epoch 30, iter 1 loss: 0.001\n",
      "epoch 30, iter 6 loss: 0.001\n",
      "epoch 30, iter 11 loss: 0.002\n",
      "epoch 30, iter 16 loss: 0.001\n",
      "epoch 30, iter 21 loss: 0.001\n",
      "epoch 30, iter 26 loss: 0.001\n",
      "epoch 30, iter 31 loss: 0.001\n",
      "epoch 30, iter 36 loss: 0.001\n",
      "epoch 30, iter 41 loss: 0.001\n",
      "epoch 31, iter 1 loss: 0.001\n",
      "epoch 31, iter 6 loss: 0.001\n",
      "epoch 31, iter 11 loss: 0.001\n",
      "epoch 31, iter 16 loss: 0.001\n",
      "epoch 31, iter 21 loss: 0.001\n",
      "epoch 31, iter 26 loss: 0.001\n",
      "epoch 31, iter 31 loss: 0.001\n",
      "epoch 31, iter 36 loss: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, iter 41 loss: 0.001\n",
      "epoch 32, iter 1 loss: 0.000\n",
      "epoch 32, iter 6 loss: 0.001\n",
      "epoch 32, iter 11 loss: 0.001\n",
      "epoch 32, iter 16 loss: 0.001\n",
      "epoch 32, iter 21 loss: 0.001\n",
      "epoch 32, iter 26 loss: 0.001\n",
      "epoch 32, iter 31 loss: 0.001\n",
      "epoch 32, iter 36 loss: 0.001\n",
      "epoch 32, iter 41 loss: 0.001\n",
      "epoch 33, iter 1 loss: 0.001\n",
      "epoch 33, iter 6 loss: 0.001\n",
      "epoch 33, iter 11 loss: 0.001\n",
      "epoch 33, iter 16 loss: 0.001\n",
      "epoch 33, iter 21 loss: 0.001\n",
      "epoch 33, iter 26 loss: 0.001\n",
      "epoch 33, iter 31 loss: 0.001\n",
      "epoch 33, iter 36 loss: 0.001\n",
      "epoch 33, iter 41 loss: 0.001\n",
      "epoch 34, iter 1 loss: 0.001\n",
      "epoch 34, iter 6 loss: 0.001\n",
      "epoch 34, iter 11 loss: 0.001\n",
      "epoch 34, iter 16 loss: 0.001\n",
      "epoch 34, iter 21 loss: 0.001\n",
      "epoch 34, iter 26 loss: 0.001\n",
      "epoch 34, iter 31 loss: 0.001\n",
      "epoch 34, iter 36 loss: 0.000\n",
      "epoch 34, iter 41 loss: 0.001\n",
      "epoch 35, iter 1 loss: 0.001\n",
      "epoch 35, iter 6 loss: 0.001\n",
      "epoch 35, iter 11 loss: 0.001\n",
      "epoch 35, iter 16 loss: 0.001\n",
      "epoch 35, iter 21 loss: 0.001\n",
      "epoch 35, iter 26 loss: 0.001\n",
      "epoch 35, iter 31 loss: 0.001\n",
      "epoch 35, iter 36 loss: 0.001\n",
      "epoch 35, iter 41 loss: 0.001\n",
      "epoch 36, iter 1 loss: 0.001\n",
      "epoch 36, iter 6 loss: 0.001\n",
      "epoch 36, iter 11 loss: 0.001\n",
      "epoch 36, iter 16 loss: 0.000\n",
      "epoch 36, iter 21 loss: 0.001\n",
      "epoch 36, iter 26 loss: 0.001\n",
      "epoch 36, iter 31 loss: 0.001\n",
      "epoch 36, iter 36 loss: 0.000\n",
      "epoch 36, iter 41 loss: 0.000\n",
      "epoch 37, iter 1 loss: 0.001\n",
      "epoch 37, iter 6 loss: 0.001\n",
      "epoch 37, iter 11 loss: 0.001\n",
      "epoch 37, iter 16 loss: 0.001\n",
      "epoch 37, iter 21 loss: 0.001\n",
      "epoch 37, iter 26 loss: 0.001\n",
      "epoch 37, iter 31 loss: 0.001\n",
      "epoch 37, iter 36 loss: 0.000\n",
      "epoch 37, iter 41 loss: 0.001\n",
      "epoch 38, iter 1 loss: 0.000\n",
      "epoch 38, iter 6 loss: 0.000\n",
      "epoch 38, iter 11 loss: 0.000\n",
      "epoch 38, iter 16 loss: 0.001\n",
      "epoch 38, iter 21 loss: 0.001\n",
      "epoch 38, iter 26 loss: 0.001\n",
      "epoch 38, iter 31 loss: 0.001\n",
      "epoch 38, iter 36 loss: 0.000\n",
      "epoch 38, iter 41 loss: 0.000\n",
      "epoch 39, iter 1 loss: 0.001\n",
      "epoch 39, iter 6 loss: 0.001\n",
      "epoch 39, iter 11 loss: 0.001\n",
      "epoch 39, iter 16 loss: 0.001\n",
      "epoch 39, iter 21 loss: 0.000\n",
      "epoch 39, iter 26 loss: 0.000\n",
      "epoch 39, iter 31 loss: 0.000\n",
      "epoch 39, iter 36 loss: 0.001\n",
      "epoch 39, iter 41 loss: 0.001\n",
      "epoch 40, iter 1 loss: 0.001\n",
      "epoch 40, iter 6 loss: 0.000\n",
      "epoch 40, iter 11 loss: 0.000\n",
      "epoch 40, iter 16 loss: 0.001\n",
      "epoch 40, iter 21 loss: 0.000\n",
      "epoch 40, iter 26 loss: 0.001\n",
      "epoch 40, iter 31 loss: 0.001\n",
      "epoch 40, iter 36 loss: 0.001\n",
      "epoch 40, iter 41 loss: 0.001\n",
      "epoch 41, iter 1 loss: 0.000\n",
      "epoch 41, iter 6 loss: 0.001\n",
      "epoch 41, iter 11 loss: 0.001\n",
      "epoch 41, iter 16 loss: 0.001\n",
      "epoch 41, iter 21 loss: 0.000\n",
      "epoch 41, iter 26 loss: 0.001\n",
      "epoch 41, iter 31 loss: 0.001\n",
      "epoch 41, iter 36 loss: 0.001\n",
      "epoch 41, iter 41 loss: 0.001\n",
      "epoch 42, iter 1 loss: 0.001\n",
      "epoch 42, iter 6 loss: 0.001\n",
      "epoch 42, iter 11 loss: 0.001\n",
      "epoch 42, iter 16 loss: 0.001\n",
      "epoch 42, iter 21 loss: 0.000\n",
      "epoch 42, iter 26 loss: 0.001\n",
      "epoch 42, iter 31 loss: 0.001\n",
      "epoch 42, iter 36 loss: 0.001\n",
      "epoch 42, iter 41 loss: 0.000\n",
      "epoch 43, iter 1 loss: 0.539\n",
      "epoch 43, iter 6 loss: 0.044\n",
      "epoch 43, iter 11 loss: 0.016\n",
      "epoch 43, iter 16 loss: 0.009\n",
      "epoch 43, iter 21 loss: 0.008\n",
      "epoch 43, iter 26 loss: 0.004\n",
      "epoch 43, iter 31 loss: 0.013\n",
      "epoch 43, iter 36 loss: 0.006\n",
      "epoch 43, iter 41 loss: 0.012\n",
      "epoch 44, iter 1 loss: 0.178\n",
      "epoch 44, iter 6 loss: 0.036\n",
      "epoch 44, iter 11 loss: 0.071\n",
      "epoch 44, iter 16 loss: 0.005\n",
      "epoch 44, iter 21 loss: 0.004\n",
      "epoch 44, iter 26 loss: 0.002\n",
      "epoch 44, iter 31 loss: 0.004\n",
      "epoch 44, iter 36 loss: 0.008\n",
      "epoch 44, iter 41 loss: 0.002\n",
      "epoch 45, iter 1 loss: 0.007\n",
      "epoch 45, iter 6 loss: 0.003\n",
      "epoch 45, iter 11 loss: 0.005\n",
      "epoch 45, iter 16 loss: 0.003\n",
      "epoch 45, iter 21 loss: 0.003\n",
      "epoch 45, iter 26 loss: 0.003\n",
      "epoch 45, iter 31 loss: 0.002\n",
      "epoch 45, iter 36 loss: 0.003\n",
      "epoch 45, iter 41 loss: 0.003\n",
      "epoch 46, iter 1 loss: 0.051\n",
      "epoch 46, iter 6 loss: 0.002\n",
      "epoch 46, iter 11 loss: 0.002\n",
      "epoch 46, iter 16 loss: 0.002\n",
      "epoch 46, iter 21 loss: 0.002\n",
      "epoch 46, iter 26 loss: 0.002\n",
      "epoch 46, iter 31 loss: 0.003\n",
      "epoch 46, iter 36 loss: 0.001\n",
      "epoch 46, iter 41 loss: 0.001\n",
      "epoch 47, iter 1 loss: 0.002\n",
      "epoch 47, iter 6 loss: 0.001\n",
      "epoch 47, iter 11 loss: 0.564\n",
      "epoch 47, iter 16 loss: 0.021\n",
      "epoch 47, iter 21 loss: 0.016\n",
      "epoch 47, iter 26 loss: 0.004\n",
      "epoch 47, iter 31 loss: 0.017\n",
      "epoch 47, iter 36 loss: 0.012\n",
      "epoch 47, iter 41 loss: 0.005\n",
      "epoch 48, iter 1 loss: 0.012\n",
      "epoch 48, iter 6 loss: 0.005\n",
      "epoch 48, iter 11 loss: 0.004\n",
      "epoch 48, iter 16 loss: 0.002\n",
      "epoch 48, iter 21 loss: 0.004\n",
      "epoch 48, iter 26 loss: 0.002\n",
      "epoch 48, iter 31 loss: 0.003\n",
      "epoch 48, iter 36 loss: 0.001\n",
      "epoch 48, iter 41 loss: 0.031\n",
      "epoch 49, iter 1 loss: 0.006\n",
      "epoch 49, iter 6 loss: 0.003\n",
      "epoch 49, iter 11 loss: 0.002\n",
      "epoch 49, iter 16 loss: 0.004\n",
      "epoch 49, iter 21 loss: 0.002\n",
      "epoch 49, iter 26 loss: 0.002\n",
      "epoch 49, iter 31 loss: 0.001\n",
      "epoch 49, iter 36 loss: 0.001\n",
      "epoch 49, iter 41 loss: 0.002\n",
      "epoch 50, iter 1 loss: 0.004\n",
      "epoch 50, iter 6 loss: 0.002\n",
      "epoch 50, iter 11 loss: 0.001\n",
      "epoch 50, iter 16 loss: 0.001\n",
      "epoch 50, iter 21 loss: 0.001\n",
      "epoch 50, iter 26 loss: 0.002\n",
      "epoch 50, iter 31 loss: 0.002\n",
      "epoch 50, iter 36 loss: 0.001\n",
      "epoch 50, iter 41 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "num_epoch = 50\n",
    "best_loss = 1e8\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        #print(data.shape,target)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 5==0:\n",
    "            print('epoch %d, iter %d loss: %.3f' %(epoch+1, batch_idx+1, loss.item()))\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                save_model(net, \"weight.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model,filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model\n",
    "net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "net = load_model(net,\"weight.pth\")\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-808e1db26f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ea92f75741a8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11025\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# rescale to -1 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# rescale to -1 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# MFCC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         ])\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# Build a Mel filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1820\u001b[0;31m     \u001b[0mmel_basis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/librosa/filters.py\u001b[0m in \u001b[0;36mmel\u001b[0;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# lower and upper slopes for all bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mramps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfdiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mramps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfdiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(valloader):\n",
    "        print(type(data))\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    acc = correct.item() / len(valloader.dataset)\n",
    "print(\"Validation Classification Accuracy: %f\"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2387\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('./data/test.npy', allow_pickle=True)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_spec = []\n",
    "for t in test_data:\n",
    "    test = cut(t)\n",
    "    test = norm(test)\n",
    "    test = spct(test) \n",
    "    test = totensor(test)\n",
    "    t_spec.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test))\n",
    "print(type(test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0f34243deb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_spec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "tensor_x = torch.stack(t_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(63.4451)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_x[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(tensor_x) # create your datset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, ) in enumerate(test_dataloader):  \n",
    "        data = data.to(device)\n",
    "        #target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        result = result + list(pred.cpu().numpy().ravel())\n",
    "    #acc = correct.item() / len(valloader.dataset)\n",
    "#print(\"Validation Classification Accuracy: %f\"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [corrected_idx2idx[idx] for idx in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 2, 18, 8, 8, 12, 2, 15, 2, 8, 2, 2, 18, 18, 11, 17, 17, 15, 2, 0, 15, 8, 5, 0, 12, 0, 3, 16, 3, 2, 18, 13, 8, 5, 18, 11, 2, 0, 15, 13, 2, 6, 14, 11, 13, 12, 0, 14, 12, 0, 5, 4, 2, 18, 15, 0, 7, 4, 18, 6, 14, 14, 17, 0, 8, 7, 6, 12, 11, 13, 18, 13, 17, 3, 12, 2, 3, 12, 12, 17, 2, 15, 14, 15, 9, 15, 7, 2, 5, 6, 5, 2, 18, 8, 0, 2, 8, 5, 1, 3, 3, 2, 0, 8, 8, 12, 15, 0, 2, 2, 5, 2, 6, 18, 1, 2, 14, 15, 12, 14, 12, 4, 0, 8, 5, 11, 15, 4, 12, 15, 7, 8, 2, 12, 15, 0, 14, 4, 0, 2, 10, 18, 4, 15, 4, 4, 15, 7, 11, 18, 3, 4, 4, 5, 11, 1, 15, 0, 7, 4, 18, 3, 3, 0, 5, 18, 14, 8, 11, 16, 2, 14, 2, 12, 1, 2, 11, 1, 1, 11, 0, 16, 10, 6, 18, 15, 5, 5, 5, 12, 6, 15, 3, 7, 15, 15, 2, 17, 16, 2, 17, 11, 15, 2, 16, 15, 13, 18, 12, 11, 17, 12, 3, 18, 12, 0, 14, 18, 3, 13, 18, 3, 18, 7, 7, 17, 12, 11, 3, 7, 8, 7, 17, 8, 18, 8, 2, 6, 3, 8, 14, 3, 8, 2, 15, 4, 2, 3, 8, 5, 15, 2, 0, 0, 11, 18, 5, 15, 18, 5, 3, 6, 12, 2, 6, 3, 3, 18, 16, 1, 2, 14, 13, 18, 7, 6, 5, 3, 1, 8, 2, 8, 3, 18, 17, 4, 13, 14, 4, 5, 11, 0, 2, 4, 14, 15, 2, 1, 4, 2, 0, 17, 14, 15, 7, 4, 8, 0, 15, 1, 11, 2, 2, 11, 2, 16, 16, 18, 3, 1, 6, 8, 14, 2, 12, 14, 18, 17, 14, 0, 4, 6, 2, 12, 7, 8, 12, 6, 15, 3, 18, 12, 3, 4, 14, 15, 15, 5, 16, 12, 5, 2, 5, 8, 3, 5, 3, 11, 10, 6, 2, 13, 18, 12, 7, 3, 3, 3, 5, 12, 4, 18, 1, 0, 3, 16, 2, 18, 2, 17, 7, 10, 17, 18, 8, 2, 6, 12, 3, 18, 6, 18, 0, 3, 12, 5, 15, 15, 7, 8, 12, 2, 13, 16, 8, 12, 3, 15, 18, 14, 5, 6, 14, 18, 2, 4, 4, 4, 8, 7, 0, 1, 5, 15, 17, 3, 2, 18, 18, 8, 7, 1, 6, 2, 5, 12, 3, 4, 2, 18, 2, 15, 8, 19, 12, 18, 16, 2, 0, 14, 8, 8, 19, 15, 4, 8, 0, 4, 6, 8, 15, 12, 6, 8, 6, 19, 12, 0, 1, 0, 4, 0, 3, 18, 15, 8, 1, 1, 2, 18, 5, 11, 3, 0, 13, 16, 6, 5, 7, 11, 2, 16, 11, 8, 3, 8, 14, 15, 1, 17, 3, 17, 14, 12, 15, 3, 1, 12, 6, 2, 18, 0, 14, 7, 0, 5, 0, 0, 18, 18, 16, 0, 3, 8, 12, 18, 15, 6, 2, 12, 3, 8, 11, 3, 1, 5, 8, 13, 17, 12, 13, 2, 8, 8, 4, 15, 13, 18, 11, 12, 0, 6, 11, 15, 12, 7, 12, 18, 16, 3, 15, 18, 4, 12, 0, 15, 12, 18, 11, 3, 15, 14, 14, 12, 0, 2, 0, 18, 7, 1, 4, 5, 11, 2, 12, 2, 11, 1, 7, 13, 0, 6, 3, 7, 0, 4, 1, 5, 3, 2, 12, 11, 8, 2, 3, 1, 5, 3, 5, 13, 6, 4, 3, 18, 6, 16, 3, 2, 14, 7, 11, 2, 12, 11, 12, 5, 3, 12, 14, 0, 14, 16, 9, 4, 5, 2, 18, 12, 0, 12, 0, 16, 4, 6, 2, 3, 13, 5, 1, 12, 3, 8, 8, 8, 17, 2, 3, 3, 7, 0, 1, 15, 12, 18, 15, 0, 4, 12, 2, 6, 18, 16, 18, 0, 14, 0, 18, 3, 17, 2, 18, 7, 7, 12, 17, 0, 18, 13, 19, 13, 14, 12, 15, 18, 2, 15, 13, 16, 3, 2, 6, 15, 14, 0, 4, 11, 8, 7, 14, 8, 3, 16, 3, 1, 17, 7, 2, 15, 16, 16, 8, 7, 14, 6, 14, 7, 14, 1, 2, 2, 0, 7, 3, 7, 0, 13, 5, 14, 11, 15, 16, 2, 3, 15, 3, 15, 15, 8, 2, 8, 2, 16, 6, 1, 0, 13, 18, 8, 2, 8, 18, 2, 5, 16, 18, 4, 12, 6, 11, 7, 12, 11, 7, 6, 3, 1, 5, 2, 1, 18, 1, 5, 15, 8, 15, 6, 6, 18, 3, 3, 2, 1, 12, 4, 6, 7, 14, 0, 4, 12, 13, 12, 18, 12, 2, 1, 11, 3, 18, 0, 1, 15, 15, 6, 17, 13, 15, 2, 15, 11, 8, 7, 13, 4, 8, 16, 15, 7, 14, 2, 18, 7, 7, 8, 5, 2, 7, 15, 4, 12, 4, 7, 13, 2, 18, 6, 18, 2, 5, 16, 13, 1, 17, 2, 1, 14, 18, 0, 1, 5, 1, 16, 18, 18, 6, 4, 12, 7, 4, 8, 12, 2, 8, 0, 14, 14, 3, 18, 13, 15, 0, 8, 0, 15, 2, 2, 14, 14, 8, 15, 16, 11, 14, 5, 18, 8, 18, 15, 7, 6, 3, 7, 5, 18, 15, 17, 11, 1, 18, 6, 3, 5, 15, 3, 17, 7, 2, 1, 0, 15, 8, 15, 18, 15, 12, 0, 12, 17, 16, 16, 8, 8, 13, 2, 3, 16, 0, 18, 12, 16, 6, 13, 12, 18, 9, 6, 0, 3, 1, 2, 3, 13, 10, 1, 6, 4, 8, 7, 14, 14, 12, 2, 12, 8, 18, 18, 18, 7, 1, 11, 7, 11, 0, 6, 11, 17, 3, 12, 18, 6, 7, 2, 17, 7, 18, 15, 0, 4, 3, 1, 12, 6, 6, 6, 15, 3, 14, 5, 15, 0, 4, 15, 16, 1, 15, 14, 12, 2, 13, 12, 0, 16, 14, 8, 16, 16, 4, 15, 8, 1, 12, 7, 8, 7, 15, 2, 2, 2, 3, 2, 6, 11, 16, 4, 18, 1, 2, 1, 6, 18, 15, 15, 4, 0, 7, 7, 11, 2, 3, 4, 18, 3, 12, 15, 3, 5, 11, 7, 2, 0, 3, 11, 2, 15, 4, 3, 3, 8, 18, 12, 0, 7, 0, 16, 15, 2, 1, 3, 0, 9, 4, 17, 2, 7, 17, 17, 0, 14, 17, 12, 0, 2, 15, 1, 7, 5, 14, 8, 14, 2, 17, 1, 17, 15, 2, 6, 18, 16, 2, 6, 1, 3, 16, 17, 3, 6, 18, 5, 15, 6, 0, 11, 8, 16, 13, 5, 15, 12, 18, 3, 15, 8, 2, 8, 15, 2, 11, 0, 5, 8, 6, 14, 11, 0, 3, 7, 11, 5, 0, 8, 14, 15, 13, 4, 16, 19, 2, 15, 14, 5, 2, 3, 16, 2, 8, 12, 3, 18, 16, 3, 16, 2, 4, 12, 15, 15, 6, 6, 12, 12, 2, 12, 15, 3, 18, 7, 2, 0, 3, 18, 3, 15, 2, 12, 18, 0, 9, 4, 2, 8, 0, 2, 17, 6, 5, 7, 15, 3, 3, 1, 15, 16, 15, 0, 11, 8, 1, 3, 2, 17, 2, 3, 17, 6, 3, 13, 15, 2, 15, 1, 16, 15, 17, 3, 8, 14, 4, 5, 18, 18, 5, 15, 0, 14, 18, 7, 15, 17, 1, 17, 15, 8, 18, 17, 3, 3, 16, 4, 4, 18, 2, 12, 12, 0, 18, 15, 2, 7, 0, 3, 15, 16, 0, 17, 7, 1, 11, 8, 16, 8, 5, 13, 18, 3, 7, 3, 12, 11, 0, 12, 3, 5, 0, 18, 16, 0, 15, 7, 18, 18, 12, 16, 11, 6, 6, 18, 13, 12, 2, 5, 2, 8, 4, 3, 14, 5, 3, 13, 5, 0, 4, 12, 17, 12, 5, 5, 18, 4, 3, 5, 3, 5, 1, 4, 16, 18, 2, 12, 14, 18, 16, 11, 18, 1, 12, 18, 3, 5, 5, 5, 0, 12, 11, 14, 12, 15, 17, 3, 12, 7, 12, 3, 15, 16, 15, 3, 2, 1, 2, 18, 5, 14, 7, 4, 15, 4, 0, 1, 5, 15, 1, 6, 11, 12, 8, 18, 7, 17, 0, 2, 11, 12, 18, 5, 18, 8, 17, 17, 14, 18, 6, 12, 11, 18, 8, 17, 0, 17, 12, 2, 6, 12, 2, 8, 2, 14, 14, 2, 18, 18, 16, 14, 3, 14, 5, 2, 10, 12, 15, 14, 15, 16, 5, 3, 5, 11, 3, 3, 8, 16, 15, 12, 15, 15, 3, 2, 12, 16, 2, 5, 12, 3, 15, 14, 18, 4, 18, 6, 7, 5, 2, 5, 3, 2, 16, 4, 15, 16, 12, 18, 16, 14, 1, 7, 14, 19, 14, 12, 0, 12, 0, 3, 0, 8, 15, 17, 12, 18, 8, 14, 3, 3, 16, 0, 7, 6, 6, 5, 17, 2, 13, 2, 7, 6, 2, 12, 4, 2, 13, 4, 14, 14, 13, 16, 13, 15, 15, 13, 0, 18, 15, 16, 0, 15, 6, 16, 7, 6, 15, 11, 8, 14, 0, 15, 8, 1, 12, 16, 4, 11, 8, 16, 16, 0, 0, 18, 15, 0, 2, 18, 1, 18, 3, 13, 18, 18, 2, 2, 18, 8, 12, 1, 3, 3, 5, 14, 6, 3, 4, 14, 6, 6, 15, 12, 16, 12, 11, 15, 4, 18, 6, 6, 2, 11, 18, 5, 12, 1, 6, 12, 0, 18, 16, 8, 7, 15, 3, 12, 8, 12, 15, 0, 2, 15, 16, 15, 0, 3, 18, 11, 15, 3, 11, 3, 5, 2, 8, 3, 3, 13, 4, 5, 13, 8, 15, 12, 18, 3, 4, 8, 15, 16, 14, 11, 0, 15, 3, 16, 18, 17, 16, 2, 11, 7, 3, 15, 12, 18, 3, 18, 17, 14, 12, 11, 5, 14, 13, 18, 5, 2, 7, 17, 12, 12, 6, 18, 7, 11, 18, 14, 15, 18, 1, 0, 12, 11, 6, 3, 12, 6, 12, 12, 4, 1, 16, 12, 8, 11, 3, 4, 12, 15, 14, 3, 16, 0, 18, 2, 5, 4, 18, 2, 18, 5, 16, 18, 3, 2, 14, 1, 7, 11, 2, 15, 7, 1, 12, 17, 18, 3, 2, 8, 15, 5, 7, 5, 2, 7, 18, 15, 6, 8, 16, 0, 2, 5, 16, 1, 12, 12, 2, 3, 4, 16, 5, 3, 8, 3, 15, 3, 18, 2, 0, 5, 1, 2, 15, 3, 14, 3, 16, 15, 18, 14, 5, 4, 2, 5, 0, 14, 12, 1, 12, 8, 0, 13, 16, 15, 3, 3, 2, 3, 8, 18, 11, 15, 7, 13, 18, 3, 2, 7, 6, 12, 11, 1, 15, 4, 4, 15, 14, 5, 11, 19, 13, 0, 12, 15, 2, 12, 5, 8, 18, 5, 18, 5, 6, 15, 3, 2, 7, 12, 2, 18, 2, 1, 14, 16, 4, 1, 12, 15, 12, 15, 15, 1, 18, 5, 1, 3, 3, 18, 13, 8, 4, 2, 2, 18, 3, 15, 18, 14, 18, 15, 15, 2, 1, 17, 12, 3, 2, 5, 16, 13, 2, 0, 2, 8, 12, 0, 7, 13, 13, 6, 4, 16, 12, 18, 6, 4, 12, 15, 10, 5, 18, 15, 11, 14, 0, 18, 13, 2, 19, 18, 15, 6, 18, 8, 6, 1, 7, 13, 2, 0, 5, 14, 0, 1, 14, 4, 2, 2, 8, 15, 18, 14, 2, 1, 7, 12, 2, 3, 17, 3, 7, 2, 0, 11, 2, 2, 18, 7, 1, 2, 15, 13, 15, 13, 6, 14, 18, 2, 14, 0, 8, 12, 18, 13, 8, 7, 18, 11, 3, 12, 15, 12, 1, 12, 7, 14, 2, 18, 2, 5, 2, 16, 15, 14, 12, 16, 16, 3, 5, 18, 3, 3, 13, 17, 15, 15, 8, 13, 0, 7, 12, 18, 3, 7, 18, 0, 7, 14, 4, 16, 12, 1, 1, 8, 11, 0, 9, 7, 1, 15, 13, 2, 4, 3, 7, 3, 12, 18, 16, 2, 0, 15, 11, 2, 7, 18, 13, 13, 8, 5, 3, 6, 18, 17, 0, 2, 17, 8, 18, 18, 7, 17, 10, 7, 12, 0, 13, 14, 0, 8, 3, 0, 17, 18, 3, 15, 19, 0, 18, 5, 8, 3, 4, 15, 3, 12, 2, 12, 15, 15, 1, 5, 18, 12, 18, 11, 18, 6, 12, 3, 12, 13, 5, 18, 18, 14, 15, 12, 15, 2, 6, 5, 11, 3, 0, 2, 16, 6, 17, 2, 2, 12, 7, 3, 3, 5, 2, 16, 12, 4, 4, 6, 18, 3, 0, 15, 14, 1, 4, 15, 8, 2, 2, 8, 14, 2, 18, 4, 11, 14, 18, 13, 4, 18, 2, 12, 12, 18, 2, 7, 14, 12, 2, 15, 0, 3, 5, 15, 3, 2, 3, 3, 16, 17, 16, 12, 2, 8, 18, 4, 3, 18, 6, 0, 13, 12, 12, 4, 2, 2, 15, 16, 4, 0, 15, 1, 11, 15, 11, 18, 4, 8, 14, 4, 4, 7, 14, 15, 18, 15, 5, 8, 11, 1, 3, 11, 18, 3, 15, 7, 13, 2, 4, 17, 4, 14, 8, 18, 2, 17, 16, 16, 7, 11, 0, 0, 8, 15, 12, 11, 3, 2, 12, 2, 11, 11, 12, 15, 12, 5, 18, 17, 6, 18, 16, 2, 6, 0, 16, 18, 0, 1, 12, 12, 8, 18, 18, 3, 8, 5, 19, 2, 7, 15, 18, 3, 5, 3, 17, 0, 13, 18, 3, 5, 14, 5, 1, 5, 11, 13, 15, 12, 18, 4, 4, 18, 3, 2, 14, 3, 2, 2, 8, 2, 0, 14, 2, 7, 2, 1, 3, 17, 12, 18, 2, 2, 4, 10, 12, 1, 2, 15, 13, 17, 0, 12, 1, 19, 11, 1, 13, 1, 14, 5, 12, 3, 12, 0, 12, 6, 13, 5, 4, 11, 0, 12, 3, 3, 12, 0, 5, 3, 14, 2, 5, 15, 8, 0, 2, 18, 18, 5, 8, 0, 13, 4, 15, 11, 12, 15, 18, 2, 13, 5, 15, 6, 11, 1, 16, 4, 2, 11, 18, 4, 18, 0, 6, 16, 16, 14, 8, 16, 14, 0, 1, 6, 3, 18, 6, 6, 2, 4, 0, 2, 0, 14, 0, 5, 7, 0, 5, 17, 8]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'id': list(range(0 ,len(test_data))), 'category':result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  category\n",
       "0   0        18\n",
       "1   1         2\n",
       "2   2        18\n",
       "3   3         8\n",
       "4   4         8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to obtain predictions from a few images\n",
    "Note. There are many ways to do the same things such as customized dataset*. The below is just a simple example how I make an inference on a few images.\n",
    "\n",
    "* https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch filepaths of the testing images\n",
    "testpath = os.path.join(dataset,\"test\") # test set\n",
    "testlist = [os.path.join(testpath,imgpath) for imgpath in os.listdir(testpath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImagePath => PIL_Image => Tensor\n",
    "from PIL import Image\n",
    "testdata = []\n",
    "for imgpath in testlist:\n",
    "    # In torchvision, we assume input images are all PIL types\n",
    "    img = Image.open(imgpath).convert(\"RGB\") # By default, torchvision read images in RGB-fashion\n",
    "    transimg = mytransforms(img)\n",
    "    testdata.append(transimg)\n",
    "testdata = torch.stack(testdata)# list of tensors to tensor\n",
    "testdata = torch.utils.data.TensorDataset(testdata)\n",
    "# =========================================================================\n",
    "# Don't shuffle the image list and set the batch_size = 1\n",
    "# It's just a trick. You can still figure out another way to achieve the same thing.\n",
    "testloader = torch.utils.data.DataLoader(testdata,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "net.eval()\n",
    "result = {}\n",
    "with torch.no_grad():\n",
    "    for idx, (data,) in enumerate(testloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred_idx = output.data.max(1, keepdim=True)[1]\n",
    "        pred_class = idx_to_class[pred_idx.cpu().numpy()[0][0]]\n",
    "        print(\"predict\",testlist[idx],\"=>\",pred_class)\n",
    "        result[testlist[idx]] = pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your results\n",
    "import matplotlib.pyplot as plt\n",
    "size = 8\n",
    "fig = plt.figure(figsize=(size, size))\n",
    "columns = len(result)\n",
    "rows = np.ceil(len(result))\n",
    "for x, filepath in enumerate(result):\n",
    "    img = plt.imread(filepath)\n",
    "    ax = fig.add_subplot(rows, columns, x+1)\n",
    "    ax.set_title(\"pred:%s\"%(result[filepath]))\n",
    "    plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
