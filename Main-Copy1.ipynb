{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.4\n",
      "1.0.1.post2\n",
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "# import some libraries you maybe use\n",
    "import torchvision # an useful library to help I/O (highly recommend). To install this, just do \"pip install torchvision\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "import os\n",
    "import visdom\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './results'\n",
    "TRAINING_NAME = 'resnet50_mag_decay_norm'\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, TRAINING_NAME)\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, 'weight.pth')\n",
    "ACC_MODEL_PATH = os.path.join(OUTPUT_DIR, 'weight_acc.pth')\n",
    "RESULT_PATH = os.path.join(OUTPUT_DIR, 'result.csv')\n",
    "NP_PATH =  os.path.join(OUTPUT_DIR, 'raw_result.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "In order to train the model with training data, the first step is to read the data from your folder, database, etc. The below is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Resize, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# Define path to your dataset\n",
    "dataset = \"./data\" # the root folder\n",
    "trainpath = os.path.join(dataset,\"train\") # train set\n",
    "valpath = os.path.join(dataset,\"val\") # validation set\n",
    "\n",
    "cut = lambda x: x[-11025:]\n",
    "cut_front = lambda x: x[:11025]\n",
    "norm =  lambda x: (x.astype(np.float32) / (np.max(x)+1e-6))*0.5\n",
    "spct = lambda x: scipy.signal.spectrogram(x ,fs= 10e3,mode='magnitude')[2] #overlap\n",
    "tri = lambda x: [x, x, x]\n",
    "totensor = lambda x: torch.Tensor(x)\n",
    "\n",
    "tsfm = Compose([\n",
    "        cut, # rescale to -1 to 1\n",
    "        norm, # rescale to -1 to 1\n",
    "        spct, # MFCC \n",
    "        tri,\n",
    "        totensor\n",
    "        ])\n",
    "\n",
    "tsfm_front = Compose([\n",
    "        cut_front, # rescale to -1 to 1\n",
    "        norm, # rescale to -1 to 1\n",
    "        spct, # MFCC \n",
    "        tri,\n",
    "        totensor\n",
    "        ])\n",
    "nploader = np.load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DatasetFolder(root=trainpath, loader=nploader, transform=tsfm, extensions=['npy'])\n",
    "valdata = DatasetFolder(root=valpath, loader=nploader, transform=tsfm_front, extensions=['npy'])\n",
    "\n",
    "# Create a loader\n",
    "trainloader = DataLoader(traindata,batch_size=batch_size,shuffle=True, pin_memory=True, num_workers=6)\n",
    "valloader = DataLoader(valdata,batch_size=batch_size,shuffle=True,  pin_memory=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frog1', 'Frog2', 'Frog3', 'Grylloidea1', 'Grylloidea2', 'Tettigonioidea1', 'Tettigonioidea2', 'drums_FloorTom', 'drums_HiHat', 'drums_Kick', 'drums_MidTom', 'drums_Ride', 'drums_Rim', 'drums_SmallTom', 'drums_Snare', 'guitar_3rd_fret', 'guitar_7th_fret', 'guitar_9th_fret', 'guitar_chord1', 'guitar_chord2']\n",
      "{'Frog1': 0, 'Frog2': 1, 'Frog3': 2, 'Grylloidea1': 3, 'Grylloidea2': 4, 'Tettigonioidea1': 5, 'Tettigonioidea2': 6, 'drums_FloorTom': 7, 'drums_HiHat': 8, 'drums_Kick': 9, 'drums_MidTom': 10, 'drums_Ride': 11, 'drums_Rim': 12, 'drums_SmallTom': 13, 'drums_Snare': 14, 'guitar_3rd_fret': 15, 'guitar_7th_fret': 16, 'guitar_9th_fret': 17, 'guitar_chord1': 18, 'guitar_chord2': 19}\n"
     ]
    }
   ],
   "source": [
    "print(traindata.classes) # show all classes\n",
    "print(traindata.class_to_idx) # show the mapping from class to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Frog1', 1: 'Frog2', 2: 'Frog3', 3: 'Grylloidea1', 4: 'Grylloidea2', 5: 'Tettigonioidea1', 6: 'Tettigonioidea2', 7: 'drums_FloorTom', 8: 'drums_HiHat', 9: 'drums_Kick', 10: 'drums_MidTom', 11: 'drums_Ride', 12: 'drums_Rim', 13: 'drums_SmallTom', 14: 'drums_Snare', 15: 'guitar_3rd_fret', 16: 'guitar_7th_fret', 17: 'guitar_9th_fret', 18: 'guitar_chord1', 19: 'guitar_chord2'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_class = {val: key for key, val in traindata.class_to_idx.items()} # build an inverse mapping for later use\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx2class = {9: 'Frog1', 10: 'Frog2', 19: 'Frog3', 3: 'Grylloidea1', 14: 'Grylloidea2', 0: 'Tettigonioidea1', 1: 'Tettigonioidea2', 11: 'drums_FloorTom', 5: 'drums_HiHat', 6: 'drums_Kick', 4: 'drums_MidTom', 16: 'drums_Ride', 13: 'drums_Rim', 7: 'drums_SmallTom', 2: 'drums_Snare', 15: 'guitar_3rd_fret', 12: 'guitar_7th_fret', 18: 'guitar_9th_fret', 17: 'guitar_chord1', 8: 'guitar_chord2'}\n",
    "#print(correct_idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class2idx = {val: key for key, val in correct_idx2class.items()}\n",
    "#print(correct_class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 10, 2: 19, 3: 3, 4: 14, 5: 0, 6: 1, 7: 11, 8: 5, 9: 6, 10: 4, 11: 16, 12: 13, 13: 7, 14: 2, 15: 15, 16: 12, 17: 18, 18: 17, 19: 8}\n"
     ]
    }
   ],
   "source": [
    "corrected_idx2idx = {val: correct_class2idx[key] for key, val in traindata.class_to_idx.items()}\n",
    "print(corrected_idx2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an example network\n",
    "If you're unfamiliar with this part, please see the HW1 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.resnet as resnet\n",
    "model =resnet.resnet50(num_classes= len(traindata.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out  = F.interpolate(out, size=(5, 5), mode='bilinear')  # resize to the size expected by the linear unit\n",
    "        out = out.view(out.size(0), 5 * 5 * 16)\n",
    "\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda now!\n"
     ]
    }
   ],
   "source": [
    "net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) # setup your optimizer\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "criterion = nn.CrossEntropyLoss() # setup your criterion\n",
    "\n",
    "lambda1 = lambda epoch: epoch // 15\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) #0.1->0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,filename):\n",
    "    state = model.state_dict()\n",
    "    for key in state: state[key] = state[key].clone().cpu()\n",
    "    torch.save(state, filename)\n",
    "#save_model(net,\"weight.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import utils.visdoms as visdoms\n",
    "plotter = visdoms.VisdomLinePlotter(env_name=TRAINING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................Val Acc: 0.054647\n",
      "epoch 1, lr 0.010000, loss: 1.5029, val_loss: 4.5162\n",
      "..........................................Val Acc: 0.082616\n",
      "epoch 2, lr 0.010000, loss: 2.7956, val_loss: 2.7272\n",
      "..........................................Val Acc: 0.088640\n",
      "epoch 3, lr 0.010000, loss: 2.7771, val_loss: 2.8580\n",
      "..........................................Val Acc: 0.219880\n",
      "epoch 4, lr 0.010000, loss: 2.3458, val_loss: 2.3786\n",
      "..........................................Val Acc: 0.334337\n",
      "epoch 5, lr 0.010000, loss: 1.8567, val_loss: 2.0515\n",
      "..........................................Val Acc: 0.472031\n",
      "epoch 6, lr 0.010000, loss: 1.3762, val_loss: 1.8493\n",
      "..........................................Val Acc: 0.581756\n",
      "epoch 7, lr 0.010000, loss: 0.9436, val_loss: 1.1201\n",
      "..........................................Val Acc: 0.710843\n",
      "epoch 8, lr 0.010000, loss: 0.5798, val_loss: 1.0488\n",
      "..........................................Val Acc: 0.798623\n",
      "epoch 9, lr 0.010000, loss: 0.3210, val_loss: 0.6953\n",
      "..........................................Val Acc: 0.776678\n",
      "epoch 10, lr 0.009000, loss: 0.3040, val_loss: 0.8332\n",
      "..........................................Val Acc: 0.879088\n",
      "epoch 11, lr 0.009000, loss: 0.2082, val_loss: 0.3852\n",
      "..........................................Val Acc: 0.886403\n",
      "epoch 12, lr 0.009000, loss: 0.1980, val_loss: 0.3581\n",
      "..........................................Val Acc: 0.911360\n",
      "epoch 13, lr 0.009000, loss: 0.2335, val_loss: 0.5114\n",
      "..........................................Val Acc: 0.922117\n",
      "epoch 14, lr 0.009000, loss: 0.1701, val_loss: 0.1267\n",
      "..........................................Val Acc: 0.942771\n",
      "epoch 15, lr 0.009000, loss: 0.1194, val_loss: 0.2253\n",
      "..........................................Val Acc: 0.934165\n",
      "epoch 16, lr 0.009000, loss: 0.1067, val_loss: 0.2785\n",
      "..........................................Val Acc: 0.943201\n",
      "epoch 17, lr 0.009000, loss: 0.1656, val_loss: 0.0886\n",
      "..........................................Val Acc: 0.929432\n",
      "epoch 18, lr 0.009000, loss: 0.0549, val_loss: 0.1533\n",
      "..........................................Val Acc: 0.921687\n",
      "epoch 19, lr 0.009000, loss: 0.0331, val_loss: 0.1551\n",
      "..........................................Val Acc: 0.917384\n",
      "epoch 20, lr 0.008100, loss: 0.2155, val_loss: 0.3263\n",
      "..........................................Val Acc: 0.940620\n",
      "epoch 21, lr 0.008100, loss: 0.0417, val_loss: 0.2002\n",
      "..........................................Val Acc: 0.963425\n",
      "epoch 22, lr 0.008100, loss: 0.0195, val_loss: 0.1134\n",
      "..........................................Val Acc: 0.949225\n",
      "epoch 23, lr 0.008100, loss: 0.0257, val_loss: 0.1010\n",
      "..........................................Val Acc: 0.961274\n",
      "epoch 24, lr 0.008100, loss: 0.0213, val_loss: 0.1856\n",
      "..........................................Val Acc: 0.947935\n",
      "epoch 25, lr 0.008100, loss: 0.0393, val_loss: 0.3107\n",
      "..........................................Val Acc: 0.962565\n",
      "epoch 26, lr 0.008100, loss: 0.0579, val_loss: 0.3317\n",
      "..........................................Val Acc: 0.965146\n",
      "epoch 27, lr 0.008100, loss: 0.0132, val_loss: 0.0410\n",
      "..........................................Val Acc: 0.974613\n",
      "epoch 28, lr 0.008100, loss: 0.0370, val_loss: 0.0367\n",
      "..........................................Val Acc: 0.954819\n",
      "epoch 29, lr 0.008100, loss: 0.0336, val_loss: 0.2811\n",
      "..........................................Val Acc: 0.864028\n",
      "epoch 30, lr 0.007290, loss: 0.2062, val_loss: 0.4951\n",
      "..........................................Val Acc: 0.965577\n",
      "epoch 31, lr 0.007290, loss: 0.0568, val_loss: 0.1077\n",
      "..........................................Val Acc: 0.964716\n",
      "epoch 32, lr 0.007290, loss: 0.0616, val_loss: 0.1918\n",
      "..........................................Val Acc: 0.966007\n",
      "epoch 33, lr 0.007290, loss: 0.0231, val_loss: 0.0177\n",
      "..........................................Val Acc: 0.972892\n",
      "epoch 34, lr 0.007290, loss: 0.0038, val_loss: 0.1094\n",
      ".......................................Val Acc: 0.968589\n",
      "epoch 35, lr 0.007290, loss: 0.0009, val_loss: 0.0040\n",
      "..........................................Val Acc: 0.966867\n",
      "epoch 36, lr 0.007290, loss: 0.0132, val_loss: 0.1594\n",
      "..........................................Val Acc: 0.975473\n",
      "epoch 37, lr 0.007290, loss: 0.0223, val_loss: 0.0019\n",
      "..........................................Val Acc: 0.968158\n",
      "epoch 38, lr 0.007290, loss: 0.0133, val_loss: 0.0261\n",
      "..........................................Val Acc: 0.919535\n",
      "epoch 39, lr 0.007290, loss: 0.0266, val_loss: 0.3675\n",
      "..........................................Val Acc: 0.964286\n",
      "epoch 40, lr 0.006561, loss: 0.0079, val_loss: 0.2787\n",
      "..........................................Val Acc: 0.960843\n",
      "epoch 41, lr 0.006561, loss: 0.0132, val_loss: 0.3183\n",
      "..........................................Val Acc: 0.969019\n",
      "epoch 42, lr 0.006561, loss: 0.0091, val_loss: 0.0790\n",
      "..........................................Val Acc: 0.974613\n",
      "epoch 43, lr 0.006561, loss: 0.0062, val_loss: 0.0883\n",
      "..........................................Val Acc: 0.981497\n",
      "epoch 44, lr 0.006561, loss: 0.0031, val_loss: 0.0897\n",
      "..........................................Val Acc: 0.964286\n",
      "epoch 45, lr 0.006561, loss: 0.0385, val_loss: 0.2378\n",
      "..........................................Val Acc: 0.970310\n",
      "epoch 46, lr 0.006561, loss: 0.0171, val_loss: 0.1398\n",
      "..........................................Val Acc: 0.980637\n",
      "epoch 47, lr 0.006561, loss: 0.0747, val_loss: 0.0720\n",
      "..........................................Val Acc: 0.958692\n",
      "epoch 48, lr 0.006561, loss: 0.0142, val_loss: 0.4741\n",
      "..........................................Val Acc: 0.973752\n",
      "epoch 49, lr 0.006561, loss: 0.0329, val_loss: 0.0758\n",
      "..........................................Val Acc: 0.976334\n",
      "epoch 50, lr 0.005905, loss: 0.0076, val_loss: 0.1431\n",
      "..........................................Val Acc: 0.978916\n",
      "epoch 51, lr 0.005905, loss: 0.0065, val_loss: 0.1505\n",
      "........................"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "num_epoch = 100\n",
    "best_loss = 1e8\n",
    "best_acc = 0\n",
    "train_losses = visdoms.AverageMeter()\n",
    "val_losses = visdoms.AverageMeter()\n",
    "for epoch in range(num_epoch):\n",
    "    #net.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        #print(data.shape,target)\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_losses.update(loss.data.cpu().numpy(), target.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('.',  end='')\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    plotter.plot('loss', 'train', 'Class Loss', epoch, train_losses.avg)\n",
    "    \n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(valloader):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            output = net(data)\n",
    "            val_loss = criterion(output, target)\n",
    "            val_losses.update(val_loss.data.cpu().numpy(), target.size(0))\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            print('.',  end='')\n",
    "        acc = correct.item() / len(valloader.dataset)\n",
    "        print(\"Val Acc: %f\"%(acc))\n",
    "\n",
    "        plotter.plot('loss', 'val', 'Class Loss', epoch, val_losses.avg)\n",
    "        plotter.plot('acc', 'val', 'Class Accuracy', epoch, acc)\n",
    "            \n",
    "        print('epoch %d, lr %.6f, loss: %.4f, val_loss: %.4f' %(epoch+1, optimizer.param_groups[0]['lr'], loss.item(), val_loss.item()))\n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            save_model(net, MODEL_PATH)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            save_model(net, ACC_MODEL_PATH)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAa7klEQVR4nO3dfXRU9b3v8fc3zyYECRBoJMVgSxVBhBiQlhZFwIIeq6Uci7Y96NXSYs+1tr3riLpuga7juvYuL/V4W3Vh1XJOqcrBWr1VOT7BsqyF1CCIICioUSMI4TnymIfv/WM2IUAmmSSTzOzN57UWa/b+7d+e/Z29hk/2/GbP3ubuiIhItGSkugAREUk+hbuISAQp3EVEIkjhLiISQQp3EZEIyurOjfXt29fLysq6c5MiIqG3evXqne5e3J51ujXcy8rKqKys7M5NioiEnpl91N51NCwjIhJBCncRkQhSuIuIRFC3jrm3pK6ujurqag4fPpzqUiIhLy+P0tJSsrOzU12KiKRQysO9urqawsJCysrKMLNUlxNq7s6uXbuorq5m0KBBqS5HRFIo5cMyhw8fpk+fPgr2JDAz+vTpo09BIpL6cAcU7EmkfSkikCbh3h5H6huoPVyX6jJERNJa6ML93c9q+XDngaQ93969e3nggQfavd4VV1zB3r17W+3zy1/+kpdffrmjpYmIdFjowj3Z4oV7Q0NDq+s9//zz9OrVq9U+v/rVr5g4cWKn6hMR6YjTPtxnz57N+++/z4gRIxg1ahTjx4/n+uuv54ILLgDgmmuu4aKLLmLo0KEsWLCgab2ysjJ27txJVVUVQ4YM4Yc//CFDhw7l8ssv59ChQwDccMMNLFmypKn/nDlzKC8v54ILLmDTpk0A1NTUMGnSJMrLy/nRj37E2Wefzc6dO7t5L4hI1KT8VMjm5v2/DbyzdX+rfQ4cqQegIDex0s8/qydzrhoad/k999zD+vXrWbt2LcuXL+fKK69k/fr1TacSPvroo/Tu3ZtDhw4xatQovvOd79CnT58TnmPz5s08/vjjPPzww1x77bU89dRTfP/73z9lW3379uXNN9/kgQce4N577+X3v/898+bN47LLLuOOO+5g6dKlJ/wBERHpqNP+yP1ko0ePPuEc8fvvv58LL7yQMWPG8Mknn7B58+ZT1hk0aBAjRowA4KKLLqKqqqrF5546deopfVasWMH06dMBmDx5MkVFRUl8NSJyukqrI/fWjrCPWVcd+xJzeGnr490dVVBQ0DS9fPlyXn75ZVauXEl+fj6XXnppi+eQ5+bmNk1nZmY2DcvE65eZmUl9fewTiG5QLiJd4bQ/ci8sLKS2trbFZfv27aOoqIj8/Hw2bdrE66+/nvTtf/3rX2fx4sUAvPjii+zZsyfp2xCR009aHbmnQp8+fRg7dizDhg3jjDPOoH///k3LJk+ezEMPPcTw4cM599xzGTNmTNK3P2fOHK677jqefPJJLrnkEkpKSigsLEz6dkTk9GLdOSxQUVHhJ9+sY+PGjQwZMiTh5+jqYZnuduTIETIzM8nKymLlypXMmjWLtWvXduo527tPRSS9mdlqd69ozzqn/ZF7qn388cdce+21NDY2kpOTw8MPP5zqkkQkAhTuKTZ48GDWrFmT6jJEJGJO+y9URUSiKKEjdzOrAmqBBqDe3SvMrDfwJFAGVAHXurtO9RARSQPtOXIf7+4jmg3qzwZecffBwCvBvIiIpIHODMtcDSwMphcC13S+HBERSYZEw92BF81stZnNDNr6u/s2gOCxX0srmtlMM6s0s8qamprOV5xiPXr0AGDr1q1MmzatxT6XXnopJ5/yebL77ruPgwcPNs0ncglhEZFEJRruY929HJgC/MTMxiW6AXdf4O4V7l5RXFzcoSLT0VlnndV0xceOODncE7mEsIhIohIKd3ffGjzuAJ4GRgPbzawEIHjc0VVFdqXbb7/9hOu5z507l3nz5jFhwoSmy/M+88wzp6xXVVXFsGHDADh06BDTp09n+PDhfPe73z3h2jKzZs2ioqKCoUOHMmfOHCB2MbKtW7cyfvx4xo8fDxy/hDDA/PnzGTZsGMOGDeO+++5r2l68SwuLiJyszbNlzKwAyHD32mD6cuBXwLPADOCe4PHUBGyvF2bDZ2+32uWc4JK/JHjJX75wAUy5J+7i6dOnc9ttt3HLLbcAsHjxYpYuXcrPfvYzevbsyc6dOxkzZgzf+ta34t6f9MEHHyQ/P59169axbt06ysvLm5bdfffd9O7dm4aGBiZMmMC6deu49dZbmT9/PsuWLaNv374nPNfq1at57LHHWLVqFe7OxRdfzCWXXEJRUVHClxYWEUnkyL0/sMLM3gL+Djzn7kuJhfokM9sMTArmQ2fkyJHs2LGDrVu38tZbb1FUVERJSQl33nknw4cPZ+LEiXz66ads37497nO89tprTSE7fPhwhg8f3rRs8eLFlJeXM3LkSDZs2MA777zTaj0rVqzg29/+NgUFBfTo0YOpU6fyt7/9DUj80sIiIm0e/rr7B8CFLbTvAiYktZpWjrCP+aALri0zbdo0lixZwmeffcb06dNZtGgRNTU1rF69muzsbMrKylq81G9zLR3Vf/jhh9x777288cYbFBUVccMNN7T5PK1d6yfRSwuLiOgXqsSGZp544gmWLFnCtGnT2LdvH/369SM7O5tly5bx0Ucftbr+uHHjWLRoEQDr169n3bp1AOzfv5+CggLOPPNMtm/fzgsvvNC0TrxLDY8bN46//OUvHDx4kAMHDvD000/zjW98I4mvVkROB7q2DDB06FBqa2sZMGAAJSUlfO973+Oqq66ioqKCESNGcN5557W6/qxZs7jxxhsZPnw4I0aMYPTo0QBceOGFjBw5kqFDh3LOOecwduzYpnVmzpzJlClTKCkpYdmyZU3t5eXl3HDDDU3PcfPNNzNy5EgNwYhIu+iSvxGkS/6KREtHLvmrYRkRkQhSuIuIRFBahLtuEp082pciAmkQ7nl5eezatUuhlATuzq5du8jLy0t1KSKSYik/W6a0tJTq6moSvajY9j2xc7s31p7RlWWFVl5eHqWlpakuQ0RSLOXhnp2dzaBBgxLuP2X2cwBU3XNlV5UkIhJ6KR+WERGR5FO4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIISDnczyzSzNWb212B+kJmtMrPNZvakmeV0XZkiItIe7Tly/ymwsdn8r4HfuPtgYA9wUzILExGRjkso3M2sFLgS+H0wb8BlwJKgy0Lgmq4oUERE2i/RI/f7gH8BGoP5PsBed68P5quBAS2taGYzzazSzCpramo6VayIiCSmzXA3s38Adrj76ubNLXT1ltZ39wXuXuHuFcXFxR0sU0RE2iMrgT5jgW+Z2RVAHtCT2JF8LzPLCo7eS4GtXVemiIi0R5tH7u5+h7uXunsZMB141d2/BywDpgXdZgDPdFmVIiLSLp05z/124OdmtoXYGPwjySlJREQ6K5FhmSbuvhxYHkx/AIxOfkkiItJZ+oWqiEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEICm24u3uqSxARSVuhDXcREYlP4S4iEkEKdxGRCAptuGvIXUQkvtCGu4iIxKdwFxGJoDbD3czyzOzvZvaWmW0ws3lB+yAzW2Vmm83sSTPL6fpyRUQkEYkcuR8BLnP3C4ERwGQzGwP8GviNuw8G9gA3dV2Zp9KQu4hIfG2Gu8d8HsxmB/8cuAxYErQvBK7pkgpFRKTdEhpzN7NMM1sL7ABeAt4H9rp7fdClGhgQZ92ZZlZpZpU1NTXJqFlERNqQULi7e4O7jwBKgdHAkJa6xVl3gbtXuHtFcXFxxysVEZGEtetsGXffCywHxgC9zCwrWFQKbE1uaW3W0p2bExEJlUTOlik2s17B9BnARGAjsAyYFnSbATzTVUWKiEj7ZLXdhRJgoZllEvtjsNjd/2pm7wBPmNm/AmuAR7qwThERaYc2w93d1wEjW2j/gNj4u4iIpJnQ/kJVI+4iIvGFNtxFRCQ+hbuISAQp3EVEIii04a7T3EVE4gttuIuISHwKdxGRCFK4i4hEUGjD3XWmu4hIXKENdxERiU/hLiISQaENd50KKSISX2jDXURE4lO4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiKLThrvPcRUTiC224i4hIfAp3EZEIUriLiERQaMNdl/wVEYkvtOEuIiLxKdxFRCJI4S4iEkGhDXed5y4iEl+b4W5mXzSzZWa20cw2mNlPg/beZvaSmW0OHou6vlwREUlEIkfu9cAv3H0IMAb4iZmdD8wGXnH3wcArwbyIiKSBNsPd3be5+5vBdC2wERgAXA0sDLotBK7pqiJFRKR92jXmbmZlwEhgFdDf3bdB7A8A0C/OOjPNrNLMKmtqajpXbTMachcRiS/hcDezHsBTwG3uvj/R9dx9gbtXuHtFcXFxR2oUEZF2SijczSybWLAvcvc/B83bzawkWF4C7OiaEkVEpL0SOVvGgEeAje4+v9miZ4EZwfQM4JnklyciIh2RlUCfscAPgLfNbG3QdidwD7DYzG4CPgb+sWtKbJnrRHcRkbjaDHd3XwFYnMUTkluOiIgkQ2h/oSoiIvEp3EVEIii04a4RdxGR+EIb7iIiEp/CXUQkghTuIiIRFNpw12nuIiLxhTbcRUQkPoW7iEgEKdxFRCIovOGuMXcRkbjCG+4iIhKXwl1EJIIU7iIiERTacHcNuouIxBXacBcRkfgU7iIiEaRwFxGJoNCGu64tIyISX2jDXURE4lO4i4hEUGjDXaMyIiLxhTbcRUQkPoW7iEgEKdxFRCIotOHuOhdSRCSu0Ia7iIjE12a4m9mjZrbDzNY3a+ttZi+Z2ebgsahryxQRkfZI5Mj9D8Dkk9pmA6+4+2DglWBeRETSRJvh7u6vAbtPar4aWBhMLwSuSXJdbdKIu4hIfB0dc+/v7tsAgsd+8Tqa2UwzqzSzypqamg5uTkRE2qPLv1B19wXuXuHuFcXFxV29ORERoePhvt3MSgCCxx3JK0lERDqro+H+LDAjmJ4BPJOcchKn09xFROJL5FTIx4GVwLlmVm1mNwH3AJPMbDMwKZgXEZE0kdVWB3e/Ls6iCUmuRUREkkS/UBURiaDQhrvrTHcRkbhCG+4iIhKfwl1EJIIU7iIiERTecNeQu4hIXOENdxERiUvhLiISQQp3EZEICm24a8hdRCS+0Ia7iIjEp3AXEYmgcIT7X38Gc8+EAztTXYmISCiEI9wrH409vvAvTU31jRp1FxGJJxzh/vONscfS0U1N1bsPpqgYEZH0F45wzz4jmDh+tP7dBa+nphYRkRAIR7hbZuyxsSG1dYiIhEQ4wj0juGFUY31q6xARCYmQhPuxI3eFu4hIItq8h2paOHbkvvJ3lGbdTXX9mQDc/8pmvtAzj0HFBRTl5/Dx7gN8qbgHpUX5ZGZYCgsWEUmtcIS7BR8wDu1mRdYsyur/BMD8l95rsXt+TiaLf/RVhg04s7sqFBFJK6EYltl/pJ69I3/cNP9P/T5gavmAuP0PHm3gH/7vCg7X6QtYETk9mXv3/RiooqLCKysr27WOu3PVb1ew/tP9/On8VXztg3+jgQwy5+wGa3no5ct3Pn/Cj5zenns5hXnZnapdRCRVzGy1u1e0Z520P3I3M34x6VwArn/nYgAyaYRF0+Ku896/TuGy8/o1zV8w90XKZj/H0vXburZYEZE0kfbhDjD+vH7M+OrZAKxsOD/WuOVlaGj57JmMDOORGRU8NetrJ7T/+I9vMviu57u0VhGRdBCKcAeYd/Uw3rhrIj+uu+14Y8ORuP3NjIvOLqLqniv5w42jmtrrGpyy2c/x6d5DABytb2TX5/GfR0QkjNJ+zP1kB4/Wk/PqHLJe/22s4da10HtQQuvOf/Fd7n91S4vLNt89hezM0PytE5HTSCTH3E+Wn5NF1sjrjzfcPwL+cktC6/788nP50w8vbnHZ4Lte4K6n305Gicmz6314/cFUVyEiIdSpcDezyWb2rpltMbPZySqqTf2HwtSHj8+vXRS73vuzt8KxTyKNjXD0wCmrfu1Lfdl89xSq8q6nKu968jg+JLNo1ccMm/2frPifX+O82X+mbPZzlM1+jtfeq4l1+LwG3njk+DY+exs+XX18/tAe2LomNt1QH2vfvgFW/i7WVrsddjb75FB/JNY2/3yofOx43QANdfDYFFg6G/Zvg70fx9p3vX98eyIicXR4WMbMMoH3gElANfAGcJ27vxNvnWQMy5xgrn6k1OT2jzicVUhdQyN7D9bRr2cuuz4/SkOjk5udweGjjRTmZdErPxsLTiF1dw7VNbDqw91cdHYRmWa8umkH9738Hn+8+WL6F+ZhFrt2fnZmBo2NztGGRrIyjKxODGEdrmto+gVx1c4DfLH38V8UH9tORjt+Ybz/cB0945zqevBoPdv3H2Fg73wAfvDIKhoanUU3X0xmhjXtC5F01pFhmc6E+1eBue7+zWD+DgB3/1/x1kl6uAPs3Ay/bddrjqRfHP0xTzWOS7h/QU4mB452/EdehblZ1B6Jna1U1iefw3WNfH6knkZ38rIz6ZEb+/FzXUMjR+obOXCknt4FObjDZ/sPt/n8RfnZnHlGNg3u7DtYR6NDz7yspj8qDY1OTlYGtYfr2Pn5UQAyDIoLc2lodHZ+fpQ+BTnsOnC0zW317ZFDQW4WRuyL+EZ3MszIMBT+3Sjqe/qRGaMY2Ce/Q+t2JNw7c/mBAcAnzeargVMGtM1sJjATYODAgZ3YXBx9B8PcfbHpw/tg/1bY/QEUlUGfwXB4L2x6DgpLwBtg7Z9g019h7E+hoB/sqYI3mg3xWAZM+CV8eRJ8sgqe+zkADV+5ksz3nsOLBmF7PgSgsVcZGXurWiyr1npQ6J+f0l7jZ3KEbErt1FsGvsdAvsLHfJT7FRoP7WNv7ll8Zv0YxQYyMjKoPFDMzqwvcKiukYO5/ZhYt4x/q5/KVX23UTpgPKxpPax7F+QwsHc+ew4eZdKQ/lTvOcTSDZ+d0McMvtKvkMK8LAb2zmf5ezXsPnCUiUP68fLGHU39ppYPYM/BOmpqj9C3MBcDqvcc5IycTLIyMuiRl0V2hpGTlUFuVib1jU5dQyMNjc7Taz6lX2EuO2pjQ2IjB/Zizcd7ARjUtwD3WHAPKelJhhm1h+vZf6iO3OwM+hTkAJBhRn1j7JPHS+9sJ8Ng5MAivlzcg8xMY+X7uxhSUshL72ynrsGDdeDYb9sqzi5i/+E6crIy+ELPPApys3CP3THAiD026m5f3caJ/r7Oyererzg7c+T+j8A33f3mYP4HwGh3/+/x1umSI3cRkYjr7rNlqoEvNpsvBbZ24vlERCRJOhPubwCDzWyQmeUA04Fnk1OWiIh0RofH3N293sz+GfgvIBN41N03JK0yERHpsE5dz93dnwd0sRYRkTQTul+oiohI2xTuIiIRpHAXEYkghbuISAR16yV/zawG+KiDq/cFTv1ZZ3pTzV0vbPWCau4uYau5tXrPdvfi9jxZt4Z7Z5hZZXt/oZVqqrnrha1eUM3dJWw1J7teDcuIiESQwl1EJILCFO4LUl1AB6jmrhe2ekE1d5ew1ZzUekMz5i4iIokL05G7iIgkSOEuIhJBoQj3lN2Iuw1mVmVmb5vZWjOrDNp6m9lLZrY5eCwK2s3M7g9ewzozK++mGh81sx1mtr5ZW7trNLMZQf/NZjYjBTXPNbNPg3291syuaLbsjqDmd83sm83au+V9Y2ZfNLNlZrbRzDaY2U+D9rTdz63UnM77Oc/M/m5mbwU1zwvaB5nZqmCfPRlcghwzyw3mtwTLy9p6Ld1Y8x/M7MNm+3lE0J6894a7p/U/YpcTfh84B8gB3gLOT3VdQW1VQN+T2v43MDuYng38Opi+AniB2F3cxgCruqnGcUA5sL6jNQK9gQ+Cx6Jguqiba54L/I8W+p4fvCdygUHBeyWzO983QAlQHkwXErtx/PnpvJ9bqTmd97MBPYLpbGBVsP8WA9OD9oeAWcH0LcBDwfR04MnWXks31/wHYFoL/ZP23gjDkftoYIu7f+DuR4EngKtTXFNrrgYWBtMLgWuatf+7x7wO9DKzkq4uxt1fA3Z3ssZvAi+5+2533wO8BEzu5prjuRp4wt2PuPuHwBZi75lue9+4+zZ3fzOYrgU2ErvHcNru51Zqjicd9rO7N92YODv458BlwJKg/eT9fGz/LwEmmJm18lq6s+Z4kvbeCEO4t3Qj7tbehN3JgRfNbLXFbgQO0N/dt0HsPxDQL2hPp9fR3hrTpfZ/Dj6qPnpsiIM0qzn46D+S2BFaKPbzSTVDGu9nM8s0s7XADmIB9z6w193rW9h+U23B8n1An1TX7O7H9vPdwX7+jZnlnlzzSbW1u+YwhLu10JYu52+OdfdyYArwEzMb10rfdH4dx8SrMR1qfxD4EjAC2Ab8n6A9bWo2sx7AU8Bt7r6/ta4ttKVLzWm9n929wd1HELtn82hgSCvbT8uazWwYcAdwHjCK2FDL7UH3pNUchnBP2xtxu/vW4HEH8DSxN9v2Y8MtweOOoHs6vY721pjy2t19e/CfpBF4mOMfo9OiZjPLJhaSi9z9z0FzWu/nlmpO9/18jLvvBZYTG5fuZWbH7irXfPtNtQXLzyQ23JfqmicHw2Lu7keAx+iC/RyGcE/LG3GbWYGZFR6bBi4H1hOr7dg32TOAZ4LpZ4F/Cr4NHwPsO/aRPQXaW+N/AZebWVHwMf3yoK3bnPT9xLeJ7etjNU8PzowYBAwG/k43vm+CcdxHgI3uPr/ZorTdz/FqTvP9XGxmvYLpM4CJxL4rWAZMC7qdvJ+P7f9pwKse+3Yy3mvprpo3Nfujb8S+I2i+n5Pz3kjGN8Jd/Y/YN8jvERtfuyvV9QQ1nUPsG/e3gA3H6iI2pvcKsDl47O3HvzX/XfAa3gYquqnOx4l9vK4j9tf/po7UCPw3Yl88bQFuTEHN/xHUtC74D1DSrP9dQc3vAlO6+30DfJ3YR+R1wNrg3xXpvJ9bqTmd9/NwYE1Q23rgl0H7OcTCeQvwn0Bu0J4XzG8Jlp/T1mvpxppfDfbzeuCPHD+jJmnvDV1+QEQkgsIwLCMiIu2kcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRND/B89lLlOd1NvpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses.arr)\n",
    "plt.plot(val_losses.arr)\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model,filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model\n",
    "net = Net(num_classes=len(traindata.classes)) # initialize your network\n",
    "net = model\n",
    "net = load_model(net, MODEL_PATH)\n",
    "# Whether to use GPU or not?\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: \n",
    "    device = 'cpu'\n",
    "print(\"use\",device,\"now!\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Classification Accuracy: 0.981928\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(valloader):\n",
    "        #print(type(data))\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    acc = correct.item() / len(valloader.dataset)\n",
    "print(\"Validation Classification Accuracy: %f\"%(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2387\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('./data/test.npy', allow_pickle=True)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_spec = []\n",
    "for t in test_data:\n",
    "    test = cut_front(t)\n",
    "    test = norm(test)\n",
    "    test = spct(test) \n",
    "    test = tri(test)\n",
    "    test = totensor(test)\n",
    "    t_spec.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test))\n",
    "print(type(test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.stack(t_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0017)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_x[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(tensor_x) # create your datset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "result = []\n",
    "raw_result = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, ) in enumerate(test_dataloader):  \n",
    "        data = data.to(device)\n",
    "        #target = target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        #correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        result = result + list(pred.cpu().numpy().ravel())\n",
    "        raw_result.append(list(output.cpu().numpy().ravel()))\n",
    "    #acc = correct.item() / len(valloader.dataset)\n",
    "#print(\"Validation Classification Accuracy: %f\"%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(NP_PATH, np.asarray(raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [corrected_idx2idx[idx] for idx in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'id': list(range(0 ,len(test_data))), 'category':result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(RESULT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
